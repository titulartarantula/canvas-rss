# v1.3.0 Unified Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Add `[NEW]`/`[UPDATE]` badges across all content types with granular parsing for Release Notes and Deploy Notes, and comment-based tracking for Q&A/Blog.

**Architecture:**
- Discussion tracking table stores comment counts for Q&A/Blog
- Release/Deploy notes parsed into individual features/changes with anchor-based source_ids
- Update detection via database comparison on each run
- First-run limits prevent feed flooding (Q&A/Blog: 5, Release/Deploy: 3)

**Tech Stack:** SQLite, Playwright, Python dataclasses, feedgen

---

## Overview

| Phase | Tasks | Description |
|-------|-------|-------------|
| 1 | 1-5 | Database foundation |
| 2 | 6-11 | Dataclasses for all content types |
| 3 | 12-16 | Scraper enhancements |
| 4 | 17-19 | LLM summarization |
| 5 | 20-24 | RSS builder updates |
| 6 | 25-27 | Classification & update detection |
| 7 | 28-29 | Main orchestration |
| 8 | 30 | Integration tests |

---

# Phase 1: Database Foundation

## Task 1: Add discussion_tracking Table

**Files:**
- Modify: `src/utils/database.py:30-92`
- Test: `tests/test_database.py`

**Step 1: Write failing test**

```python
class TestDiscussionTracking:
    """Tests for discussion tracking functionality."""

    def test_discussion_tracking_table_created(self, temp_db):
        """Test that discussion_tracking table is created on init."""
        conn = temp_db._get_connection()
        cursor = conn.cursor()
        cursor.execute(
            "SELECT name FROM sqlite_master WHERE type='table' AND name='discussion_tracking'"
        )
        assert cursor.fetchone() is not None

    def test_discussion_tracking_schema(self, temp_db):
        """Test that discussion_tracking has correct columns."""
        conn = temp_db._get_connection()
        cursor = conn.cursor()
        cursor.execute("PRAGMA table_info(discussion_tracking)")
        columns = {row[1] for row in cursor.fetchall()}
        expected = {"source_id", "post_type", "comment_count", "first_seen", "last_checked"}
        assert expected == columns
```

**Step 2: Run test to verify failure**

```bash
pytest tests/test_database.py::TestDiscussionTracking -v
```

**Step 3: Implement**

Add to `src/utils/database.py` in `_init_schema()` after feed_history table:

```python
        # Discussion tracking table for [NEW]/[UPDATE] badges
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS discussion_tracking (
                source_id TEXT PRIMARY KEY,
                post_type TEXT NOT NULL,
                comment_count INTEGER DEFAULT 0,
                first_seen TEXT NOT NULL,
                last_checked TEXT NOT NULL
            )
        """)
```

**Step 4: Verify pass**

```bash
pytest tests/test_database.py::TestDiscussionTracking -v
```

**Step 5: Commit**

```bash
git add src/utils/database.py tests/test_database.py
git commit -m "feat(db): add discussion_tracking table"
```

---

## Task 2: Add feature_tracking Table

**Files:**
- Modify: `src/utils/database.py`
- Test: `tests/test_database.py`

**Step 1: Write failing test**

```python
class TestFeatureTracking:
    """Tests for release/deploy feature tracking."""

    def test_feature_tracking_table_created(self, temp_db):
        """Test that feature_tracking table is created."""
        conn = temp_db._get_connection()
        cursor = conn.cursor()
        cursor.execute(
            "SELECT name FROM sqlite_master WHERE type='table' AND name='feature_tracking'"
        )
        assert cursor.fetchone() is not None

    def test_feature_tracking_schema(self, temp_db):
        """Test feature_tracking columns."""
        conn = temp_db._get_connection()
        cursor = conn.cursor()
        cursor.execute("PRAGMA table_info(feature_tracking)")
        columns = {row[1] for row in cursor.fetchall()}
        expected = {"source_id", "parent_id", "feature_type", "anchor_id", "first_seen", "last_checked"}
        assert expected == columns
```

**Step 2: Run test**

```bash
pytest tests/test_database.py::TestFeatureTracking -v
```

**Step 3: Implement**

Add to `_init_schema()`:

```python
        # Feature tracking for Release/Deploy notes granular items
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS feature_tracking (
                source_id TEXT PRIMARY KEY,
                parent_id TEXT NOT NULL,
                feature_type TEXT NOT NULL,
                anchor_id TEXT NOT NULL,
                first_seen TEXT NOT NULL,
                last_checked TEXT NOT NULL
            )
        """)
```

**Step 4: Verify pass**

```bash
pytest tests/test_database.py::TestFeatureTracking -v
```

**Step 5: Commit**

```bash
git add src/utils/database.py tests/test_database.py
git commit -m "feat(db): add feature_tracking table for release/deploy notes"
```

---

## Task 3: Add Discussion Tracking Methods

**Files:**
- Modify: `src/utils/database.py`
- Test: `tests/test_database.py`

**Step 1: Write failing tests**

```python
    def test_get_discussion_tracking_returns_none_for_unknown(self, temp_db):
        """Test get_discussion_tracking returns None for unknown source_id."""
        result = temp_db.get_discussion_tracking("unknown-id")
        assert result is None

    def test_upsert_discussion_tracking_creates_record(self, temp_db):
        """Test upsert creates new tracking record."""
        temp_db.upsert_discussion_tracking("question_123", "question", 5)
        result = temp_db.get_discussion_tracking("question_123")
        assert result is not None
        assert result["comment_count"] == 5

    def test_upsert_discussion_tracking_updates_existing(self, temp_db):
        """Test upsert updates comment_count but preserves first_seen."""
        temp_db.upsert_discussion_tracking("question_789", "question", 2)
        first = temp_db.get_discussion_tracking("question_789")

        temp_db.upsert_discussion_tracking("question_789", "question", 8)
        updated = temp_db.get_discussion_tracking("question_789")

        assert updated["comment_count"] == 8
        assert updated["first_seen"] == first["first_seen"]

    def test_is_discussion_tracking_empty(self, temp_db):
        """Test first-run detection."""
        assert temp_db.is_discussion_tracking_empty() is True
        temp_db.upsert_discussion_tracking("q_1", "question", 0)
        assert temp_db.is_discussion_tracking_empty() is False
```

**Step 2: Run tests**

```bash
pytest tests/test_database.py::TestDiscussionTracking -v
```

**Step 3: Implement**

Add to `src/utils/database.py`:

```python
    def get_discussion_tracking(self, source_id: str) -> Optional[dict]:
        """Get tracking data for a discussion post."""
        conn = self._get_connection()
        cursor = conn.cursor()
        cursor.execute(
            "SELECT source_id, post_type, comment_count, first_seen, last_checked "
            "FROM discussion_tracking WHERE source_id = ?",
            (source_id,)
        )
        row = cursor.fetchone()
        return dict(row) if row else None

    def upsert_discussion_tracking(
        self, source_id: str, post_type: str, comment_count: int
    ) -> None:
        """Insert or update tracking data for a discussion post."""
        conn = self._get_connection()
        cursor = conn.cursor()
        now = datetime.now().isoformat()

        existing = self.get_discussion_tracking(source_id)
        if existing:
            cursor.execute(
                "UPDATE discussion_tracking SET comment_count = ?, last_checked = ? WHERE source_id = ?",
                (comment_count, now, source_id)
            )
        else:
            cursor.execute(
                "INSERT INTO discussion_tracking (source_id, post_type, comment_count, first_seen, last_checked) "
                "VALUES (?, ?, ?, ?, ?)",
                (source_id, post_type, comment_count, now, now)
            )
        conn.commit()

    def is_discussion_tracking_empty(self) -> bool:
        """Check if discussion_tracking table is empty (first run)."""
        conn = self._get_connection()
        cursor = conn.cursor()
        cursor.execute("SELECT COUNT(*) FROM discussion_tracking")
        return cursor.fetchone()[0] == 0
```

**Step 4: Verify pass**

```bash
pytest tests/test_database.py::TestDiscussionTracking -v
```

**Step 5: Commit**

```bash
git add src/utils/database.py tests/test_database.py
git commit -m "feat(db): add discussion tracking methods"
```

---

## Task 4: Add Feature Tracking Methods

**Files:**
- Modify: `src/utils/database.py`
- Test: `tests/test_database.py`

**Step 1: Write failing tests**

```python
    def test_get_feature_tracking_returns_none_for_unknown(self, temp_db):
        """Test get_feature_tracking returns None for unknown."""
        result = temp_db.get_feature_tracking("unknown")
        assert result is None

    def test_upsert_feature_tracking_creates_record(self, temp_db):
        """Test upsert creates feature tracking record."""
        temp_db.upsert_feature_tracking(
            source_id="release-2026-02-21#doc-app",
            parent_id="release-2026-02-21",
            feature_type="release_note_feature",
            anchor_id="doc-app"
        )
        result = temp_db.get_feature_tracking("release-2026-02-21#doc-app")
        assert result is not None
        assert result["anchor_id"] == "doc-app"

    def test_get_features_for_parent(self, temp_db):
        """Test getting all features for a parent release/deploy."""
        temp_db.upsert_feature_tracking("release-2026-02-21#f1", "release-2026-02-21", "release_note_feature", "f1")
        temp_db.upsert_feature_tracking("release-2026-02-21#f2", "release-2026-02-21", "release_note_feature", "f2")
        temp_db.upsert_feature_tracking("release-2026-02-22#f1", "release-2026-02-22", "release_note_feature", "f1")

        features = temp_db.get_features_for_parent("release-2026-02-21")
        assert len(features) == 2

    def test_is_feature_tracking_empty(self, temp_db):
        """Test first-run detection for features."""
        assert temp_db.is_feature_tracking_empty() is True
        temp_db.upsert_feature_tracking("r#f", "r", "release_note_feature", "f")
        assert temp_db.is_feature_tracking_empty() is False
```

**Step 2: Run tests**

```bash
pytest tests/test_database.py::TestFeatureTracking -v
```

**Step 3: Implement**

```python
    def get_feature_tracking(self, source_id: str) -> Optional[dict]:
        """Get tracking data for a release/deploy feature."""
        conn = self._get_connection()
        cursor = conn.cursor()
        cursor.execute(
            "SELECT source_id, parent_id, feature_type, anchor_id, first_seen, last_checked "
            "FROM feature_tracking WHERE source_id = ?",
            (source_id,)
        )
        row = cursor.fetchone()
        return dict(row) if row else None

    def upsert_feature_tracking(
        self, source_id: str, parent_id: str, feature_type: str, anchor_id: str
    ) -> None:
        """Insert or update tracking data for a feature."""
        conn = self._get_connection()
        cursor = conn.cursor()
        now = datetime.now().isoformat()

        existing = self.get_feature_tracking(source_id)
        if existing:
            cursor.execute(
                "UPDATE feature_tracking SET last_checked = ? WHERE source_id = ?",
                (now, source_id)
            )
        else:
            cursor.execute(
                "INSERT INTO feature_tracking (source_id, parent_id, feature_type, anchor_id, first_seen, last_checked) "
                "VALUES (?, ?, ?, ?, ?, ?)",
                (source_id, parent_id, feature_type, anchor_id, now, now)
            )
        conn.commit()

    def get_features_for_parent(self, parent_id: str) -> List[dict]:
        """Get all tracked features for a parent release/deploy."""
        conn = self._get_connection()
        cursor = conn.cursor()
        cursor.execute(
            "SELECT source_id, parent_id, feature_type, anchor_id, first_seen, last_checked "
            "FROM feature_tracking WHERE parent_id = ?",
            (parent_id,)
        )
        return [dict(row) for row in cursor.fetchall()]

    def is_feature_tracking_empty(self) -> bool:
        """Check if feature_tracking table is empty (first run)."""
        conn = self._get_connection()
        cursor = conn.cursor()
        cursor.execute("SELECT COUNT(*) FROM feature_tracking")
        return cursor.fetchone()[0] == 0
```

**Step 4: Verify pass**

```bash
pytest tests/test_database.py::TestFeatureTracking -v
```

**Step 5: Commit**

```bash
git add src/utils/database.py tests/test_database.py
git commit -m "feat(db): add feature tracking methods"
```

---

## Task 5: Add Combined First-Run Detection

**Files:**
- Modify: `src/utils/database.py`
- Test: `tests/test_database.py`

**Step 1: Write failing test**

```python
    def test_is_first_run_for_type(self, temp_db):
        """Test type-specific first-run detection."""
        # All empty initially
        assert temp_db.is_first_run_for_type("question") is True
        assert temp_db.is_first_run_for_type("blog") is True
        assert temp_db.is_first_run_for_type("release_note") is True

        # Add a question
        temp_db.upsert_discussion_tracking("q_1", "question", 0)
        assert temp_db.is_first_run_for_type("question") is False
        assert temp_db.is_first_run_for_type("blog") is True  # Still empty

        # Add a release feature
        temp_db.upsert_feature_tracking("r#f", "r", "release_note_feature", "f")
        assert temp_db.is_first_run_for_type("release_note") is False
```

**Step 2: Run test**

```bash
pytest tests/test_database.py::TestDiscussionTracking::test_is_first_run_for_type -v
```

**Step 3: Implement**

```python
    def is_first_run_for_type(self, content_type: str) -> bool:
        """Check if this is the first run for a specific content type.

        Args:
            content_type: 'question', 'blog', 'release_note', or 'deploy_note'

        Returns:
            True if no items of this type have been tracked yet.
        """
        conn = self._get_connection()
        cursor = conn.cursor()

        if content_type in ("question", "blog"):
            cursor.execute(
                "SELECT COUNT(*) FROM discussion_tracking WHERE post_type = ?",
                (content_type,)
            )
        elif content_type in ("release_note", "deploy_note"):
            feature_type = f"{content_type}_feature"
            cursor.execute(
                "SELECT COUNT(*) FROM feature_tracking WHERE feature_type = ?",
                (feature_type,)
            )
        else:
            return True

        return cursor.fetchone()[0] == 0
```

**Step 4: Verify pass**

```bash
pytest tests/test_database.py -v
```

**Step 5: Commit**

```bash
git add src/utils/database.py tests/test_database.py
git commit -m "feat(db): add is_first_run_for_type method"
```

---

# Phase 2: Dataclasses

## Task 6: Add DiscussionUpdate Dataclass

**Files:**
- Modify: `src/scrapers/instructure_community.py`
- Test: `tests/test_scrapers.py`

**Step 1: Write failing test**

```python
class TestDiscussionUpdate:
    """Tests for DiscussionUpdate dataclass."""

    def test_discussion_update_new_post(self):
        """Test DiscussionUpdate for new post."""
        from scrapers.instructure_community import DiscussionUpdate, CommunityPost
        from datetime import datetime

        post = CommunityPost(
            title="Test", url="http://example.com", content="Content",
            published_date=datetime.now(), post_type="question"
        )
        update = DiscussionUpdate(
            post=post, is_new=True, previous_comment_count=0,
            new_comment_count=0, latest_comment=None
        )
        assert update.is_new is True

    def test_discussion_update_with_new_comments(self):
        """Test DiscussionUpdate for post with new comments."""
        from scrapers.instructure_community import DiscussionUpdate, CommunityPost
        from datetime import datetime

        post = CommunityPost(
            title="Test", url="http://example.com", content="Content",
            published_date=datetime.now(), comments=8, post_type="question"
        )
        update = DiscussionUpdate(
            post=post, is_new=False, previous_comment_count=5,
            new_comment_count=3, latest_comment="Latest reply..."
        )
        assert update.is_new is False
        assert update.new_comment_count == 3
```

**Step 2: Run test**

```bash
pytest tests/test_scrapers.py::TestDiscussionUpdate -v
```

**Step 3: Implement**

Add to `src/scrapers/instructure_community.py` after `CommunityPost`:

```python
@dataclass
class DiscussionUpdate:
    """Represents a discussion post that is new or has new comments."""
    post: CommunityPost
    is_new: bool
    previous_comment_count: int
    new_comment_count: int
    latest_comment: Optional[str]
```

**Step 4: Verify pass**

```bash
pytest tests/test_scrapers.py::TestDiscussionUpdate -v
```

**Step 5: Commit**

```bash
git add src/scrapers/instructure_community.py tests/test_scrapers.py
git commit -m "feat(scraper): add DiscussionUpdate dataclass"
```

---

## Task 7: Add FeatureTableData Dataclass

**Files:**
- Modify: `src/scrapers/instructure_community.py`
- Test: `tests/test_scrapers.py`

**Step 1: Write failing test**

```python
class TestFeatureTableData:
    """Tests for FeatureTableData dataclass."""

    def test_feature_table_data_creation(self):
        """Test creating FeatureTableData."""
        from scrapers.instructure_community import FeatureTableData

        table = FeatureTableData(
            enable_location="Account Settings",
            default_status="Off",
            permissions="Admin only",
            affected_areas=["Assignments", "SpeedGrader"],
            affects_roles=["instructors", "students"]
        )
        assert table.enable_location == "Account Settings"
        assert "Assignments" in table.affected_areas
```

**Step 2: Run test**

```bash
pytest tests/test_scrapers.py::TestFeatureTableData -v
```

**Step 3: Implement**

```python
@dataclass
class FeatureTableData:
    """Configuration table data for a release/deploy feature."""
    enable_location: str
    default_status: str
    permissions: str
    affected_areas: List[str]
    affects_roles: List[str]
```

**Step 4: Verify pass, commit**

```bash
git commit -am "feat(scraper): add FeatureTableData dataclass"
```

---

## Task 8: Add Feature Dataclass

**Files:**
- Modify: `src/scrapers/instructure_community.py`
- Test: `tests/test_scrapers.py`

**Step 1: Write failing test**

```python
class TestFeature:
    """Tests for Feature dataclass."""

    def test_feature_creation(self):
        """Test creating Feature."""
        from scrapers.instructure_community import Feature, FeatureTableData

        table = FeatureTableData(
            enable_location="Account", default_status="Off",
            permissions="Admin", affected_areas=["Assignments"],
            affects_roles=["instructors"]
        )
        feature = Feature(
            category="Assignments",
            name="Document Processing App",
            anchor_id="document-processing-app",
            added_date=None,
            raw_content="<p>Feature content</p>",
            table_data=table
        )
        assert feature.category == "Assignments"
        assert feature.anchor_id == "document-processing-app"

    def test_feature_source_id(self):
        """Test Feature source_id generation."""
        from scrapers.instructure_community import Feature, FeatureTableData

        table = FeatureTableData("", "", "", [], [])
        feature = Feature(
            category="Apps", name="Test", anchor_id="test-feature",
            added_date=None, raw_content="", table_data=table
        )
        # source_id should be set by parent page
        assert feature.anchor_id == "test-feature"
```

**Step 2: Run test**

```bash
pytest tests/test_scrapers.py::TestFeature -v
```

**Step 3: Implement**

```python
@dataclass
class Feature:
    """A single feature from a Release/Deploy Notes page."""
    category: str
    name: str
    anchor_id: str
    added_date: Optional[datetime]
    raw_content: str
    table_data: Optional[FeatureTableData]
```

**Step 4: Verify pass, commit**

```bash
git commit -am "feat(scraper): add Feature dataclass"
```

---

## Task 9: Add UpcomingChange Dataclass

**Files:**
- Modify: `src/scrapers/instructure_community.py`
- Test: `tests/test_scrapers.py`

**Step 1: Write failing test**

```python
class TestUpcomingChange:
    """Tests for UpcomingChange dataclass."""

    def test_upcoming_change_creation(self):
        """Test creating UpcomingChange."""
        from scrapers.instructure_community import UpcomingChange
        from datetime import datetime

        change = UpcomingChange(
            date=datetime(2026, 3, 21),
            description="User-Agent Header Enforcement",
            days_until=48
        )
        assert change.description == "User-Agent Header Enforcement"

    def test_upcoming_change_urgency(self):
        """Test urgency detection (within 30 days)."""
        from scrapers.instructure_community import UpcomingChange
        from datetime import datetime

        urgent = UpcomingChange(datetime(2026, 2, 15), "Urgent change", 14)
        not_urgent = UpcomingChange(datetime(2026, 4, 1), "Later change", 60)

        assert urgent.days_until <= 30
        assert not_urgent.days_until > 30
```

**Step 2: Run test**

```bash
pytest tests/test_scrapers.py::TestUpcomingChange -v
```

**Step 3: Implement**

```python
@dataclass
class UpcomingChange:
    """An upcoming Canvas change/deprecation."""
    date: datetime
    description: str
    days_until: int
```

**Step 4: Verify pass, commit**

```bash
git commit -am "feat(scraper): add UpcomingChange dataclass"
```

---

## Task 10: Add ReleaseNotePage Dataclass

**Files:**
- Modify: `src/scrapers/instructure_community.py`
- Test: `tests/test_scrapers.py`

**Step 1: Write failing test**

```python
class TestReleaseNotePage:
    """Tests for ReleaseNotePage dataclass."""

    def test_release_note_page_creation(self):
        """Test creating ReleaseNotePage."""
        from scrapers.instructure_community import ReleaseNotePage, Feature, FeatureTableData
        from datetime import datetime

        table = FeatureTableData("", "", "", [], [])
        feature = Feature("Apps", "Test", "test", None, "", table)

        page = ReleaseNotePage(
            title="Canvas Release Notes (2026-02-21)",
            url="https://example.com/release",
            release_date=datetime(2026, 2, 21),
            upcoming_changes=[],
            features=[feature],
            sections={"New Features": [feature]}
        )
        assert "2026-02-21" in page.title
        assert len(page.features) == 1

    def test_release_note_page_source_id(self):
        """Test source_id format for release page."""
        from scrapers.instructure_community import ReleaseNotePage
        from datetime import datetime

        page = ReleaseNotePage(
            title="Canvas Release Notes (2026-02-21)",
            url="https://example.com",
            release_date=datetime(2026, 2, 21),
            upcoming_changes=[], features=[], sections={}
        )
        # Parent source_id should be date-based
        expected_id = "release-2026-02-21"
        assert page.release_date.strftime("release-%Y-%m-%d") == expected_id
```

**Step 2: Run test**

```bash
pytest tests/test_scrapers.py::TestReleaseNotePage -v
```

**Step 3: Implement**

```python
@dataclass
class ReleaseNotePage:
    """A parsed Release Notes page with all features."""
    title: str
    url: str
    release_date: datetime
    upcoming_changes: List[UpcomingChange]
    features: List[Feature]
    sections: Dict[str, List[Feature]]
```

**Step 4: Verify pass, commit**

```bash
git commit -am "feat(scraper): add ReleaseNotePage dataclass"
```

---

## Task 11: Add DeployChange and DeployNotePage Dataclasses

**Files:**
- Modify: `src/scrapers/instructure_community.py`
- Test: `tests/test_scrapers.py`

**Step 1: Write failing test**

```python
class TestDeployNoteDataclasses:
    """Tests for Deploy Note dataclasses."""

    def test_deploy_change_creation(self):
        """Test creating DeployChange."""
        from scrapers.instructure_community import DeployChange, FeatureTableData

        change = DeployChange(
            category="Navigation",
            name="Small Screen Branding Updated",
            anchor_id="small-screen-global-navigation-branding-updated",
            section="Updated Features",
            raw_content="<p>Content</p>",
            table_data=None,
            status=None,
            status_date=None
        )
        assert change.section == "Updated Features"

    def test_deploy_change_delayed_status(self):
        """Test DeployChange with delayed status."""
        from scrapers.instructure_community import DeployChange
        from datetime import datetime

        change = DeployChange(
            category="Apps", name="Delayed Feature",
            anchor_id="delayed-feature", section="Updated Features",
            raw_content="", table_data=None,
            status="delayed", status_date=datetime(2026, 1, 30)
        )
        assert change.status == "delayed"

    def test_deploy_note_page_creation(self):
        """Test creating DeployNotePage."""
        from scrapers.instructure_community import DeployNotePage, DeployChange
        from datetime import datetime

        change = DeployChange("Nav", "Fix", "fix", "Updates", "", None, None, None)
        page = DeployNotePage(
            title="Canvas Deploy Notes (2026-02-11)",
            url="https://example.com/deploy",
            deploy_date=datetime(2026, 2, 11),
            beta_date=datetime(2026, 1, 29),
            changes=[change],
            sections={"Updated Features": [change]}
        )
        assert page.beta_date is not None
        assert len(page.changes) == 1
```

**Step 2: Run test**

```bash
pytest tests/test_scrapers.py::TestDeployNoteDataclasses -v
```

**Step 3: Implement**

```python
@dataclass
class DeployChange:
    """A single change from a Deploy Notes page."""
    category: str
    name: str
    anchor_id: str
    section: str
    raw_content: str
    table_data: Optional[FeatureTableData]
    status: Optional[str]  # "delayed", None
    status_date: Optional[datetime]


@dataclass
class DeployNotePage:
    """A parsed Deploy Notes page with all changes."""
    title: str
    url: str
    deploy_date: datetime
    beta_date: Optional[datetime]
    changes: List[DeployChange]
    sections: Dict[str, List[DeployChange]]
```

**Step 4: Verify pass, commit**

```bash
git commit -am "feat(scraper): add DeployChange and DeployNotePage dataclasses"
```

---

# Phase 3: Scraper Enhancements

## Task 12: Add extract_source_id Helper

**Files:**
- Modify: `src/scrapers/instructure_community.py`
- Test: `tests/test_scrapers.py`

**Step 1: Write failing test**

```python
class TestExtractSourceId:
    """Tests for extract_source_id helper."""

    def test_extract_from_discussion_url(self):
        """Test extracting ID from discussion URL."""
        from scrapers.instructure_community import extract_source_id
        url = "https://community.instructure.com/en/discussion/664587/test"
        assert extract_source_id(url, "question") == "question_664587"

    def test_extract_from_blog_url(self):
        """Test extracting ID from blog URL."""
        from scrapers.instructure_community import extract_source_id
        url = "https://community.instructure.com/en/blog/664752/test"
        assert extract_source_id(url, "blog") == "blog_664752"

    def test_extract_fallback_to_hash(self):
        """Test fallback to hash for non-matching URL."""
        from scrapers.instructure_community import extract_source_id
        url = "https://example.com/other/path"
        result = extract_source_id(url, "question")
        assert result.startswith("question_")
```

**Step 2: Run test**

```bash
pytest tests/test_scrapers.py::TestExtractSourceId -v
```

**Step 3: Implement**

Add at module level after imports:

```python
def extract_source_id(url: str, post_type: str) -> str:
    """Extract numeric ID from Instructure Community URL.

    Args:
        url: Full URL to a community post.
        post_type: Type of post ('question', 'blog', etc.).

    Returns:
        Source ID in format '{post_type}_{numeric_id}'.
    """
    match = re.search(r'/(discussion|blog)/(\d+)', url)
    if match:
        return f"{post_type}_{match.group(2)}"
    return f"{post_type}_{abs(hash(url))}"
```

**Step 4: Verify pass, commit**

```bash
git commit -am "feat(scraper): add extract_source_id helper"
```

---

## Task 13: Add scrape_latest_comment Method

**Files:**
- Modify: `src/scrapers/instructure_community.py`
- Test: `tests/test_scrapers.py`

**Step 1: Write failing test**

```python
class TestScrapeLatestComment:
    """Tests for scrape_latest_comment method."""

    def test_returns_none_without_browser(self):
        """Test returns None when browser not available."""
        from scrapers.instructure_community import InstructureScraper
        scraper = InstructureScraper.__new__(InstructureScraper)
        scraper.page = None
        assert scraper.scrape_latest_comment("http://example.com") is None

    def test_truncates_long_comments(self, mocker):
        """Test that long comments are truncated to 500 chars."""
        from scrapers.instructure_community import InstructureScraper

        scraper = InstructureScraper.__new__(InstructureScraper)
        scraper.rate_limit_seconds = 0

        mock_page = mocker.MagicMock()
        mock_element = mocker.MagicMock()
        mock_element.inner_text.return_value = "A" * 600
        mock_page.query_selector.return_value = mock_element
        mock_page.goto = mocker.MagicMock()
        mock_page.wait_for_load_state = mocker.MagicMock()
        scraper.page = mock_page

        result = scraper.scrape_latest_comment("http://example.com/discussion/1")
        assert len(result) == 500
```

**Step 2: Run test**

```bash
pytest tests/test_scrapers.py::TestScrapeLatestComment -v
```

**Step 3: Implement**

Add to `InstructureScraper` class:

```python
    def scrape_latest_comment(self, url: str) -> Optional[str]:
        """Navigate to a post and extract the most recent comment.

        Args:
            url: URL of the community post.

        Returns:
            Text of the latest comment (max 500 chars), or None.
        """
        if not self.page:
            return None

        try:
            self._rate_limit()
            self.page.goto(url, timeout=30000)
            self.page.wait_for_load_state("networkidle", timeout=15000)

            comment_selectors = [
                "[class*='comment']:last-child",
                "[class*='reply']:last-of-type",
                "[class*='message']:last-child",
                "[class*='Comment']:last-child",
            ]

            for selector in comment_selectors:
                try:
                    element = self.page.query_selector(selector)
                    if element:
                        text = element.inner_text().strip()
                        if text and len(text) > 10:
                            return text[:500] if len(text) > 500 else text
                except Exception:
                    continue

            return None

        except PlaywrightTimeout:
            logger.warning(f"Timeout scraping comment from: {url}")
            return None
        except Exception as e:
            logger.error(f"Error scraping comment from {url}: {e}")
            return None
```

**Step 4: Verify pass, commit**

```bash
git commit -am "feat(scraper): add scrape_latest_comment method"
```

---

## Task 14: Add parse_release_note_page Method (Stub)

**Files:**
- Modify: `src/scrapers/instructure_community.py`
- Test: `tests/test_scrapers.py`

**Step 1: Write failing test**

```python
class TestParseReleaseNotePage:
    """Tests for parse_release_note_page method."""

    def test_returns_none_without_browser(self):
        """Test returns None when browser not available."""
        from scrapers.instructure_community import InstructureScraper
        scraper = InstructureScraper.__new__(InstructureScraper)
        scraper.page = None
        assert scraper.parse_release_note_page("http://example.com") is None

    def test_parses_page_title(self, mocker):
        """Test that page title is extracted."""
        from scrapers.instructure_community import InstructureScraper

        scraper = InstructureScraper.__new__(InstructureScraper)
        scraper.rate_limit_seconds = 0

        mock_page = mocker.MagicMock()
        mock_page.title.return_value = "Canvas Release Notes (2026-02-21)"
        mock_page.goto = mocker.MagicMock()
        mock_page.wait_for_load_state = mocker.MagicMock()
        mock_page.query_selector_all.return_value = []
        mock_page.query_selector.return_value = None
        scraper.page = mock_page

        result = scraper.parse_release_note_page("http://example.com/release")
        assert result is not None
        assert "2026-02-21" in result.title
```

**Step 2: Run test**

```bash
pytest tests/test_scrapers.py::TestParseReleaseNotePage -v
```

**Step 3: Implement stub**

```python
    def parse_release_note_page(self, url: str) -> Optional[ReleaseNotePage]:
        """Parse a Release Notes page into structured data.

        Args:
            url: URL of the release notes page.

        Returns:
            ReleaseNotePage with features, or None on error.
        """
        if not self.page:
            return None

        try:
            self._rate_limit()
            self.page.goto(url, timeout=30000)
            self.page.wait_for_load_state("networkidle", timeout=15000)

            title = self.page.title() or "Canvas Release Notes"

            # Extract date from title
            date_match = re.search(r'\((\d{4}-\d{2}-\d{2})\)', title)
            if date_match:
                release_date = datetime.strptime(date_match.group(1), "%Y-%m-%d")
            else:
                release_date = datetime.now(timezone.utc)

            # TODO: Implement full parsing of features
            # For now, return empty features list
            return ReleaseNotePage(
                title=title,
                url=url,
                release_date=release_date,
                upcoming_changes=[],
                features=[],
                sections={}
            )

        except Exception as e:
            logger.error(f"Error parsing release notes from {url}: {e}")
            return None
```

**Step 4: Verify pass, commit**

```bash
git commit -am "feat(scraper): add parse_release_note_page stub"
```

---

## Task 15: Add parse_deploy_note_page Method (Stub)

**Files:**
- Modify: `src/scrapers/instructure_community.py`
- Test: `tests/test_scrapers.py`

**Step 1: Write failing test**

```python
class TestParseDeployNotePage:
    """Tests for parse_deploy_note_page method."""

    def test_returns_none_without_browser(self):
        """Test returns None when browser not available."""
        from scrapers.instructure_community import InstructureScraper
        scraper = InstructureScraper.__new__(InstructureScraper)
        scraper.page = None
        assert scraper.parse_deploy_note_page("http://example.com") is None

    def test_parses_page_title(self, mocker):
        """Test that page title is extracted."""
        from scrapers.instructure_community import InstructureScraper

        scraper = InstructureScraper.__new__(InstructureScraper)
        scraper.rate_limit_seconds = 0

        mock_page = mocker.MagicMock()
        mock_page.title.return_value = "Canvas Deploy Notes (2026-02-11)"
        mock_page.goto = mocker.MagicMock()
        mock_page.wait_for_load_state = mocker.MagicMock()
        mock_page.query_selector_all.return_value = []
        mock_page.query_selector.return_value = None
        scraper.page = mock_page

        result = scraper.parse_deploy_note_page("http://example.com/deploy")
        assert result is not None
        assert "2026-02-11" in result.title
```

**Step 2: Run test**

```bash
pytest tests/test_scrapers.py::TestParseDeployNotePage -v
```

**Step 3: Implement stub**

```python
    def parse_deploy_note_page(self, url: str) -> Optional[DeployNotePage]:
        """Parse a Deploy Notes page into structured data.

        Args:
            url: URL of the deploy notes page.

        Returns:
            DeployNotePage with changes, or None on error.
        """
        if not self.page:
            return None

        try:
            self._rate_limit()
            self.page.goto(url, timeout=30000)
            self.page.wait_for_load_state("networkidle", timeout=15000)

            title = self.page.title() or "Canvas Deploy Notes"

            # Extract date from title
            date_match = re.search(r'\((\d{4}-\d{2}-\d{2})\)', title)
            if date_match:
                deploy_date = datetime.strptime(date_match.group(1), "%Y-%m-%d")
            else:
                deploy_date = datetime.now(timezone.utc)

            # TODO: Implement full parsing of changes
            return DeployNotePage(
                title=title,
                url=url,
                deploy_date=deploy_date,
                beta_date=None,
                changes=[],
                sections={}
            )

        except Exception as e:
            logger.error(f"Error parsing deploy notes from {url}: {e}")
            return None
```

**Step 4: Verify pass, commit**

```bash
git commit -am "feat(scraper): add parse_deploy_note_page stub"
```

---

## Task 16: Implement Full Feature Parsing for Release Notes

**Files:**
- Modify: `src/scrapers/instructure_community.py`
- Test: `tests/test_scrapers.py`

**Step 1: Write failing test with mock HTML**

```python
    def test_parses_features_from_html(self, mocker):
        """Test full feature parsing from HTML structure."""
        from scrapers.instructure_community import InstructureScraper

        # Mock HTML with data-id attributes
        mock_html = """
        <h2 data-id="new-features">New Features</h2>
        <h3 data-id="assignments">Assignments</h3>
        <h4 data-id="document-processing-app">Document Processing App</h4>
        <p>Feature description here.</p>
        """

        scraper = InstructureScraper.__new__(InstructureScraper)
        scraper.rate_limit_seconds = 0

        mock_page = mocker.MagicMock()
        mock_page.title.return_value = "Canvas Release Notes (2026-02-21)"
        mock_page.goto = mocker.MagicMock()
        mock_page.wait_for_load_state = mocker.MagicMock()
        mock_page.content.return_value = mock_html

        # Mock query_selector_all to return feature elements
        mock_h4 = mocker.MagicMock()
        mock_h4.get_attribute.return_value = "document-processing-app"
        mock_h4.inner_text.return_value = "Document Processing App"
        mock_page.query_selector_all.return_value = [mock_h4]
        mock_page.query_selector.return_value = None

        scraper.page = mock_page

        result = scraper.parse_release_note_page("http://example.com/release")
        # With stub, features will be empty - full impl needed
        assert result is not None
```

**Step 2: Run test**

```bash
pytest tests/test_scrapers.py::TestParseReleaseNotePage::test_parses_features_from_html -v
```

**Step 3: Implement full parsing**

Update `parse_release_note_page` with feature extraction:

```python
    def parse_release_note_page(self, url: str) -> Optional[ReleaseNotePage]:
        """Parse a Release Notes page into structured data."""
        if not self.page:
            return None

        try:
            self._rate_limit()
            self.page.goto(url, timeout=30000)
            self.page.wait_for_load_state("networkidle", timeout=15000)

            title = self.page.title() or "Canvas Release Notes"

            date_match = re.search(r'\((\d{4}-\d{2}-\d{2})\)', title)
            release_date = datetime.strptime(date_match.group(1), "%Y-%m-%d") if date_match else datetime.now(timezone.utc)

            features = []
            sections = {}
            current_section = "New Features"
            current_category = "General"

            # Parse H2 (sections), H3 (categories), H4 (features)
            headings = self.page.query_selector_all("h2[data-id], h3[data-id], h4[data-id]")

            for heading in headings:
                tag = heading.evaluate("el => el.tagName.toLowerCase()")
                data_id = heading.get_attribute("data-id") or ""
                text = heading.inner_text().strip()

                if tag == "h2":
                    current_section = text
                    if current_section not in sections:
                        sections[current_section] = []
                elif tag == "h3":
                    current_category = text
                elif tag == "h4":
                    # Extract [Added DATE] annotation
                    added_date = None
                    added_match = re.search(r'\[Added (\d{4}-\d{2}-\d{2})\]', text)
                    if added_match:
                        added_date = datetime.strptime(added_match.group(1), "%Y-%m-%d")
                        text = re.sub(r'\s*\[Added \d{4}-\d{2}-\d{2}\]', '', text)

                    # Get content after heading (simplified)
                    raw_content = ""
                    next_sibling = heading.evaluate(
                        "el => { let s = el.nextElementSibling; return s ? s.outerHTML : ''; }"
                    )
                    if next_sibling:
                        raw_content = next_sibling

                    feature = Feature(
                        category=current_category,
                        name=text,
                        anchor_id=data_id,
                        added_date=added_date,
                        raw_content=raw_content,
                        table_data=None  # TODO: Parse configuration table
                    )
                    features.append(feature)

                    if current_section in sections:
                        sections[current_section].append(feature)

            return ReleaseNotePage(
                title=title,
                url=url,
                release_date=release_date,
                upcoming_changes=[],
                features=features,
                sections=sections
            )

        except Exception as e:
            logger.error(f"Error parsing release notes from {url}: {e}")
            return None
```

**Step 4: Verify pass, commit**

```bash
git commit -am "feat(scraper): implement full release note page parsing"
```

---

# Phase 4: LLM Summarization

## Task 17: Add summarize_feature Method

**Files:**
- Modify: `src/processor/content_processor.py`
- Test: `tests/test_processor.py`

**Step 1: Write failing test**

```python
class TestSummarizeFeature:
    """Tests for feature summarization."""

    def test_summarize_feature_prompt(self):
        """Test feature summarization uses correct prompt."""
        from processor.content_processor import ContentProcessor
        from scrapers.instructure_community import Feature, FeatureTableData

        processor = ContentProcessor(gemini_api_key=None)  # No API key = fallback

        table = FeatureTableData("Account", "Off", "Admin", ["Assignments"], ["instructors"])
        feature = Feature(
            category="Assignments",
            name="Document Processing App",
            anchor_id="doc-app",
            added_date=None,
            raw_content="This feature allows document processing in Canvas.",
            table_data=table
        )

        # Without API key, should return truncated content
        result = processor.summarize_feature(feature)
        assert "document processing" in result.lower() or result == ""
```

**Step 2: Run test**

```bash
pytest tests/test_processor.py::TestSummarizeFeature -v
```

**Step 3: Implement**

Add to `ContentProcessor`:

```python
    FEATURE_SUMMARIZATION_PROMPT = """You are summarizing a Canvas LMS feature for educational technologists.

Feature: {feature_name}
Category: {category}

Content:
{raw_content}

Write a 2-3 sentence summary that covers:
1. What this feature does
2. Who benefits from it (students, instructors, admins)
3. The key improvement or capability it provides

Keep it concise and jargon-free."""

    def summarize_feature(self, feature: "Feature") -> str:
        """Generate summary for a release/deploy feature.

        Args:
            feature: Feature dataclass with raw_content.

        Returns:
            2-3 sentence summary string.
        """
        if not feature.raw_content:
            return ""

        if self.client is None:
            return feature.raw_content[:300] if len(feature.raw_content) > 300 else feature.raw_content

        try:
            prompt = self.FEATURE_SUMMARIZATION_PROMPT.format(
                feature_name=feature.name,
                category=feature.category,
                raw_content=feature.raw_content
            )
            response = self.client.models.generate_content(
                model=self.gemini_model,
                contents=prompt,
                config=self.generation_config
            )
            return response.text.strip()[:500]
        except Exception as e:
            logger.error(f"Feature summarization failed: {e}")
            return ""
```

**Step 4: Verify pass, commit**

```bash
git commit -am "feat(processor): add summarize_feature method"
```

---

## Task 18: Add format_availability Helper

**Files:**
- Modify: `src/processor/content_processor.py`
- Test: `tests/test_processor.py`

**Step 1: Write failing test**

```python
class TestFormatAvailability:
    """Tests for format_availability helper."""

    def test_format_availability_full(self):
        """Test full availability string."""
        from processor.content_processor import format_availability
        from scrapers.instructure_community import FeatureTableData

        table = FeatureTableData(
            enable_location="Account Settings",
            default_status="Off",
            permissions="Admin only",
            affected_areas=["Assignments", "SpeedGrader"],
            affects_roles=["instructors", "students"]
        )

        result = format_availability(table)
        assert "Admin" in result
        assert "Account" in result
        assert "instructors" in result

    def test_format_availability_none_table(self):
        """Test with None table returns default."""
        from processor.content_processor import format_availability

        result = format_availability(None)
        assert "Automatic" in result
```

**Step 2: Run test**

```bash
pytest tests/test_processor.py::TestFormatAvailability -v
```

**Step 3: Implement**

Add module-level function:

```python
def format_availability(table: Optional["FeatureTableData"]) -> str:
    """Format availability summary from feature table.

    Args:
        table: FeatureTableData or None.

    Returns:
        Formatted availability string.
    """
    if table is None:
        return "Automatic update"

    parts = []

    if table.permissions and table.enable_location:
        parts.append(f"{table.permissions}-enabled at {table.enable_location.lower()}")

    if table.affects_roles:
        roles = " and ".join(table.affects_roles)
        parts.append(f"affects {roles}")

    if table.affected_areas:
        areas = ", ".join(table.affected_areas)
        parts.append(f"in {areas}")

    return "; ".join(parts) if parts else "Automatic update"
```

**Step 4: Verify pass, commit**

```bash
git commit -am "feat(processor): add format_availability helper"
```

---

## Task 19: Add summarize_deploy_change Method

**Files:**
- Modify: `src/processor/content_processor.py`
- Test: `tests/test_processor.py`

**Step 1: Write failing test**

```python
class TestSummarizeDeployChange:
    """Tests for deploy change summarization."""

    def test_summarize_deploy_change(self):
        """Test deploy change summarization."""
        from processor.content_processor import ContentProcessor
        from scrapers.instructure_community import DeployChange

        processor = ContentProcessor(gemini_api_key=None)

        change = DeployChange(
            category="Navigation",
            name="Small Screen Branding Fix",
            anchor_id="small-screen-fix",
            section="Updated Features",
            raw_content="Fixed branding display on mobile devices.",
            table_data=None,
            status=None,
            status_date=None
        )

        result = processor.summarize_deploy_change(change)
        assert "branding" in result.lower() or "mobile" in result.lower() or result == ""
```

**Step 2: Run test**

```bash
pytest tests/test_processor.py::TestSummarizeDeployChange -v
```

**Step 3: Implement**

```python
    DEPLOY_CHANGE_PROMPT = """You are summarizing a Canvas LMS change for educational technologists.

Change: {change_name}
Category: {category}
Section: {section}

Content:
{raw_content}

Write a 2-3 sentence summary that covers:
1. What behavior changed
2. Why it was changed (bug fix, improvement, accessibility, etc.)
3. Who needs to be aware of this change

Keep it concise and jargon-free."""

    def summarize_deploy_change(self, change: "DeployChange") -> str:
        """Generate summary for a deploy change.

        Args:
            change: DeployChange dataclass.

        Returns:
            2-3 sentence summary string.
        """
        if not change.raw_content:
            return ""

        if self.client is None:
            return change.raw_content[:300] if len(change.raw_content) > 300 else change.raw_content

        try:
            prompt = self.DEPLOY_CHANGE_PROMPT.format(
                change_name=change.name,
                category=change.category,
                section=change.section,
                raw_content=change.raw_content
            )
            response = self.client.models.generate_content(
                model=self.gemini_model,
                contents=prompt,
                config=self.generation_config
            )
            return response.text.strip()[:500]
        except Exception as e:
            logger.error(f"Deploy change summarization failed: {e}")
            return ""
```

**Step 4: Verify pass, commit**

```bash
git commit -am "feat(processor): add summarize_deploy_change method"
```

---

# Phase 5: RSS Builder Updates

## Task 20: Add SOURCE_LABELS Constant

**Files:**
- Modify: `src/generator/rss_builder.py`
- Test: `tests/test_rss_builder.py`

**Step 1: Write failing test**

```python
class TestSourceLabels:
    """Tests for SOURCE_LABELS constant."""

    def test_source_labels_defined(self):
        """Test SOURCE_LABELS has expected keys."""
        from generator.rss_builder import SOURCE_LABELS

        assert "question" in SOURCE_LABELS
        assert "blog" in SOURCE_LABELS
        assert SOURCE_LABELS["question"] == "Question Forum"
        assert SOURCE_LABELS["blog"] == "Blog"
```

**Step 2: Run test**

```bash
pytest tests/test_rss_builder.py::TestSourceLabels -v
```

**Step 3: Implement**

Add after existing constants:

```python
# Source labels for title formatting
SOURCE_LABELS = {
    "question": "Question Forum",
    "blog": "Blog",
    "release_note": "Release Notes",
    "deploy_note": "Deploy Notes",
}
```

**Step 4: Verify pass, commit**

```bash
git commit -am "feat(rss): add SOURCE_LABELS constant"
```

---

## Task 21: Add build_discussion_title Function

**Files:**
- Modify: `src/generator/rss_builder.py`
- Test: `tests/test_rss_builder.py`

**Step 1: Write failing test**

```python
class TestBuildDiscussionTitle:
    """Tests for build_discussion_title function."""

    def test_new_question_title(self):
        """Test [NEW] title for question."""
        from generator.rss_builder import build_discussion_title

        result = build_discussion_title("question", "How to configure SSO?", True)
        assert result == "[NEW] - Question Forum - How to configure SSO?"

    def test_update_blog_title(self):
        """Test [UPDATE] title for blog."""
        from generator.rss_builder import build_discussion_title

        result = build_discussion_title("blog", "Studio Updates", False)
        assert result == "[UPDATE] - Blog - Studio Updates"

    def test_release_note_no_source_label(self):
        """Test release notes don't get source label."""
        from generator.rss_builder import build_discussion_title

        result = build_discussion_title("release_note", "Canvas Release Notes (2026-02-21)", True)
        assert result == "[NEW] Canvas Release Notes (2026-02-21)"
```

**Step 2: Run test**

```bash
pytest tests/test_rss_builder.py::TestBuildDiscussionTitle -v
```

**Step 3: Implement**

```python
def build_discussion_title(post_type: str, title: str, is_new: bool) -> str:
    """Build title with [NEW]/[UPDATE] badge and optional source label.

    Args:
        post_type: 'question', 'blog', 'release_note', or 'deploy_note'.
        title: Original post title.
        is_new: True for [NEW], False for [UPDATE].

    Returns:
        Formatted title string.
    """
    badge = "[NEW]" if is_new else "[UPDATE]"

    if post_type in ("question", "blog"):
        source = SOURCE_LABELS.get(post_type, "")
        return f"{badge} - {source} - {title}"
    else:
        return f"{badge} {title}"
```

**Step 4: Verify pass, commit**

```bash
git commit -am "feat(rss): add build_discussion_title function"
```

---

## Task 22: Add format_discussion_description Function

**Files:**
- Modify: `src/generator/rss_builder.py`
- Test: `tests/test_rss_builder.py`

**Step 1: Write failing test**

```python
class TestFormatDiscussionDescription:
    """Tests for format_discussion_description function."""

    def test_new_question_description(self):
        """Test description for new question."""
        from generator.rss_builder import format_discussion_description

        desc = format_discussion_description(
            post_type="question", is_new=True,
            content="How do I configure SSO?",
            comment_count=0, previous_comment_count=0,
            new_comment_count=0, latest_comment=None
        )
        assert "NEW QUESTION" in desc

    def test_update_description_with_comment(self):
        """Test update description with latest comment."""
        from generator.rss_builder import format_discussion_description

        desc = format_discussion_description(
            post_type="question", is_new=False,
            content="Original question",
            comment_count=8, previous_comment_count=5,
            new_comment_count=3, latest_comment="Try this solution..."
        )
        assert "+3 new comments" in desc
        assert "8 total" in desc
        assert "Latest reply" in desc
```

**Step 2: Run test**

```bash
pytest tests/test_rss_builder.py::TestFormatDiscussionDescription -v
```

**Step 3: Implement**

```python
SECTION_HEADERS = {
    "question_new": "NEW QUESTION",
    "question_update": "DISCUSSION UPDATE",
    "blog_new": "NEW BLOG POST",
    "blog_update": "BLOG UPDATE",
}


def format_discussion_description(
    post_type: str,
    is_new: bool,
    content: str,
    comment_count: int,
    previous_comment_count: int,
    new_comment_count: int,
    latest_comment: Optional[str]
) -> str:
    """Format RSS description for a discussion post."""
    key = f"{post_type}_{'new' if is_new else 'update'}"
    header = SECTION_HEADERS.get(key, "UPDATE")

    parts = [f" {header} ", ""]

    if is_new:
        truncated = content[:300] if len(content) > 300 else content
        if len(content) > 300:
            truncated = truncated.rsplit(' ', 1)[0] + "..."
        parts.append(truncated)
        parts.append("")
        parts.append(f"Posted: {comment_count} comments")
    else:
        parts.append(f"+{new_comment_count} new comments ({comment_count} total)")
        parts.append("")

        if latest_comment:
            preview = latest_comment[:300]
            if len(latest_comment) > 300:
                preview = preview.rsplit(' ', 1)[0] + "..."
            parts.append(" Latest reply:")
            parts.append(f'"{preview}"')
            parts.append("")

        parts.append("")
        truncated = content[:200] if len(content) > 200 else content
        if len(content) > 200:
            truncated = truncated.rsplit(' ', 1)[0] + "..."
        parts.append(f"Original: {truncated}")

    return "\n".join(parts)
```

**Step 4: Verify pass, commit**

```bash
git commit -am "feat(rss): add format_discussion_description function"
```

---

## Task 23: Add build_release_note_entry Function

**Files:**
- Modify: `src/generator/rss_builder.py`
- Test: `tests/test_rss_builder.py`

**Step 1: Write failing test**

```python
class TestBuildReleaseNoteEntry:
    """Tests for build_release_note_entry function."""

    def test_new_release_note_format(self):
        """Test [NEW] release note entry."""
        from generator.rss_builder import build_release_note_entry
        from scrapers.instructure_community import ReleaseNotePage, Feature, FeatureTableData
        from datetime import datetime

        table = FeatureTableData("Account", "Off", "Admin", ["Assignments"], ["instructors"])
        feature = Feature("Assignments", "Doc App", "doc-app", None, "Content", table)
        page = ReleaseNotePage(
            title="Canvas Release Notes (2026-02-21)",
            url="http://example.com/release",
            release_date=datetime(2026, 2, 21),
            upcoming_changes=[],
            features=[feature],
            sections={"New Features": [feature]}
        )

        result = build_release_note_entry(page, is_update=False, new_features=None)
        assert "NEW FEATURES" in result
        assert "Doc App" in result
```

**Step 2: Run test**

```bash
pytest tests/test_rss_builder.py::TestBuildReleaseNoteEntry -v
```

**Step 3: Implement**

```python
def build_release_note_entry(
    page: "ReleaseNotePage",
    is_update: bool,
    new_features: Optional[List[str]] = None
) -> str:
    """Build RSS description for a release notes page.

    Args:
        page: ReleaseNotePage with parsed features.
        is_update: True if this is an update (new features added).
        new_features: List of anchor_ids for new features (updates only).

    Returns:
        Formatted description string.
    """
    parts = [f"[Full Release Notes]({page.url})", ""]

    # Filter features if update
    features_to_show = page.features
    if is_update and new_features:
        features_to_show = [f for f in page.features if f.anchor_id in new_features]

    # Group by section
    for section_name, section_features in page.sections.items():
        if is_update and new_features:
            section_features = [f for f in section_features if f.anchor_id in new_features]
        if not section_features:
            continue

        parts.append(f" {section_name.upper()} ")
        parts.append("")

        for feature in section_features:
            anchor_link = f"{page.url}#{feature.anchor_id}"
            added_tag = ""
            if feature.added_date:
                added_tag = f" [Added {feature.added_date.strftime('%Y-%m-%d')}]"

            parts.append(f" {feature.category} - [{feature.name}]({anchor_link}){added_tag}")
            parts.append(f"[Summary placeholder]")  # Will be filled by LLM
            parts.append(f"Availability: {format_availability(feature.table_data)}")
            parts.append("")

    return "\n".join(parts)
```

**Step 4: Verify pass, commit**

```bash
git commit -am "feat(rss): add build_release_note_entry function"
```

---

## Task 24: Add build_deploy_note_entry Function

**Files:**
- Modify: `src/generator/rss_builder.py`
- Test: `tests/test_rss_builder.py`

**Step 1: Write failing test**

```python
class TestBuildDeployNoteEntry:
    """Tests for build_deploy_note_entry function."""

    def test_deploy_note_with_delayed_status(self):
        """Test deploy note with delayed status flag."""
        from generator.rss_builder import build_deploy_note_entry
        from scrapers.instructure_community import DeployNotePage, DeployChange
        from datetime import datetime

        change = DeployChange(
            category="Apps", name="Delayed Feature",
            anchor_id="delayed-feature", section="Updated Features",
            raw_content="Content", table_data=None,
            status="delayed", status_date=datetime(2026, 1, 30)
        )
        page = DeployNotePage(
            title="Canvas Deploy Notes (2026-02-11)",
            url="http://example.com/deploy",
            deploy_date=datetime(2026, 2, 11),
            beta_date=datetime(2026, 1, 29),
            changes=[change],
            sections={"Updated Features": [change]}
        )

        result = build_deploy_note_entry(page, is_update=False, new_changes=None)
        assert "" in result  # Delayed flag
        assert "2026-01-30" in result
```

**Step 2: Run test**

```bash
pytest tests/test_rss_builder.py::TestBuildDeployNoteEntry -v
```

**Step 3: Implement**

```python
STATUS_FLAGS = {
    "delayed": "",
}


def build_deploy_note_entry(
    page: "DeployNotePage",
    is_update: bool,
    new_changes: Optional[List[str]] = None
) -> str:
    """Build RSS description for a deploy notes page.

    Args:
        page: DeployNotePage with parsed changes.
        is_update: True if this is an update.
        new_changes: List of anchor_ids for new changes (updates only).

    Returns:
        Formatted description string.
    """
    parts = [f"[Full Deploy Notes]({page.url})", ""]

    if page.beta_date:
        parts.append(f"Beta: {page.beta_date.strftime('%Y-%m-%d')} | Production: {page.deploy_date.strftime('%Y-%m-%d')}")
        parts.append("")

    for section_name, section_changes in page.sections.items():
        if is_update and new_changes:
            section_changes = [c for c in section_changes if c.anchor_id in new_changes]
        if not section_changes:
            continue

        parts.append(f" {section_name.upper()} ")
        parts.append("")

        for change in section_changes:
            anchor_link = f"{page.url}#{change.anchor_id}"
            status_flag = STATUS_FLAGS.get(change.status, "")

            parts.append(f"{status_flag} {change.category} - [{change.name}]({anchor_link})")
            parts.append("[Summary placeholder]")
            parts.append(f"Availability: {format_availability(change.table_data)}")

            if change.status == "delayed" and change.status_date:
                parts.append(f"Delayed: {change.status_date.strftime('%Y-%m-%d')}")

            parts.append("")

    return "\n".join(parts)
```

**Step 4: Verify pass, commit**

```bash
git commit -am "feat(rss): add build_deploy_note_entry function"
```

---

# Phase 6: Classification & Update Detection

## Task 25: Add classify_discussion_posts Function

**Files:**
- Modify: `src/scrapers/instructure_community.py`
- Test: `tests/test_scrapers.py`

**Step 1: Write failing test**

```python
class TestClassifyDiscussionPosts:
    """Tests for classify_discussion_posts function."""

    def test_new_post_classified(self, temp_db):
        """Test new posts are classified as new."""
        from scrapers.instructure_community import CommunityPost, classify_discussion_posts
        from datetime import datetime

        posts = [CommunityPost(
            title="New Q", url="http://example.com/discussion/999/test",
            content="Content", published_date=datetime.now(),
            comments=3, post_type="question"
        )]

        results = classify_discussion_posts(posts, temp_db, first_run_limit=5)
        assert len(results) == 1
        assert results[0].is_new is True

    def test_updated_post_classified(self, temp_db):
        """Test posts with new comments are classified as updates."""
        from scrapers.instructure_community import CommunityPost, classify_discussion_posts
        from datetime import datetime

        temp_db.upsert_discussion_tracking("question_888", "question", 5)

        posts = [CommunityPost(
            title="Existing Q", url="http://example.com/discussion/888/test",
            content="Content", published_date=datetime.now(),
            comments=8, post_type="question"
        )]

        results = classify_discussion_posts(posts, temp_db, first_run_limit=5)
        assert len(results) == 1
        assert results[0].is_new is False
        assert results[0].new_comment_count == 3

    def test_first_run_limit_enforced(self, temp_db):
        """Test first-run limit caps new posts."""
        from scrapers.instructure_community import CommunityPost, classify_discussion_posts
        from datetime import datetime

        posts = [
            CommunityPost(
                title=f"Q{i}", url=f"http://example.com/discussion/{i}/test",
                content="Content", published_date=datetime.now(),
                comments=0, post_type="question"
            ) for i in range(10)
        ]

        results = classify_discussion_posts(posts, temp_db, first_run_limit=3)
        assert len(results) == 3  # Limited

        # All should be tracked
        for i in range(10):
            assert temp_db.get_discussion_tracking(f"question_{i}") is not None
```

**Step 2: Run test**

```bash
pytest tests/test_scrapers.py::TestClassifyDiscussionPosts -v
```

**Step 3: Implement**

Add to `src/scrapers/instructure_community.py`:

```python
def classify_discussion_posts(
    posts: List[CommunityPost],
    db: "Database",
    first_run_limit: int = 5,
    scraper: Optional["InstructureScraper"] = None
) -> List[DiscussionUpdate]:
    """Classify posts as new or updated based on comment tracking.

    Args:
        posts: List of CommunityPost objects.
        db: Database instance for tracking.
        first_run_limit: Max new posts on first run.
        scraper: Optional scraper for fetching latest comments.

    Returns:
        List of DiscussionUpdate objects to include in feed.
    """
    results = []
    new_count = 0

    for post in posts:
        source_id = extract_source_id(post.url, post.post_type)
        tracked = db.get_discussion_tracking(source_id)

        if tracked is None:
            new_count += 1
            if new_count > first_run_limit:
                db.upsert_discussion_tracking(source_id, post.post_type, post.comments)
                continue

            results.append(DiscussionUpdate(
                post=post, is_new=True,
                previous_comment_count=0,
                new_comment_count=post.comments,
                latest_comment=None
            ))

        elif post.comments > tracked["comment_count"]:
            new_comments = post.comments - tracked["comment_count"]
            latest_comment = scraper.scrape_latest_comment(post.url) if scraper else None

            results.append(DiscussionUpdate(
                post=post, is_new=False,
                previous_comment_count=tracked["comment_count"],
                new_comment_count=new_comments,
                latest_comment=latest_comment
            ))

        db.upsert_discussion_tracking(source_id, post.post_type, post.comments)

    return results
```

**Step 4: Verify pass, commit**

```bash
git commit -am "feat(scraper): add classify_discussion_posts function"
```

---

## Task 26: Add classify_release_features Function

**Files:**
- Modify: `src/scrapers/instructure_community.py`
- Test: `tests/test_scrapers.py`

**Step 1: Write failing test**

```python
class TestClassifyReleaseFeatures:
    """Tests for classify_release_features function."""

    def test_new_features_detected(self, temp_db):
        """Test new features are detected."""
        from scrapers.instructure_community import (
            ReleaseNotePage, Feature, FeatureTableData, classify_release_features
        )
        from datetime import datetime

        feature = Feature("Apps", "New Feature", "new-feature", None, "", None)
        page = ReleaseNotePage(
            title="Canvas Release Notes (2026-02-21)",
            url="http://example.com/release",
            release_date=datetime(2026, 2, 21),
            upcoming_changes=[], features=[feature], sections={}
        )

        is_new, new_anchors = classify_release_features(page, temp_db, first_run_limit=3)
        assert is_new is True
        assert "new-feature" in new_anchors

    def test_first_run_limit_for_features(self, temp_db):
        """Test first-run limit for features."""
        from scrapers.instructure_community import (
            ReleaseNotePage, Feature, classify_release_features
        )
        from datetime import datetime

        features = [Feature("Cat", f"Feature {i}", f"f{i}", None, "", None) for i in range(5)]
        page = ReleaseNotePage(
            title="Canvas Release Notes (2026-02-21)",
            url="http://example.com/release",
            release_date=datetime(2026, 2, 21),
            upcoming_changes=[], features=features, sections={}
        )

        is_new, new_anchors = classify_release_features(page, temp_db, first_run_limit=3)
        assert len(new_anchors) == 3  # Limited

        # All should be tracked
        for i in range(5):
            assert temp_db.get_feature_tracking(f"release-2026-02-21#f{i}") is not None
```

**Step 2: Run test**

```bash
pytest tests/test_scrapers.py::TestClassifyReleaseFeatures -v
```

**Step 3: Implement**

```python
def classify_release_features(
    page: ReleaseNotePage,
    db: "Database",
    first_run_limit: int = 3
) -> Tuple[bool, List[str]]:
    """Classify release note features as new or existing.

    Args:
        page: ReleaseNotePage with parsed features.
        db: Database instance for tracking.
        first_run_limit: Max new features on first run.

    Returns:
        Tuple of (is_new_page, list_of_new_anchor_ids).
    """
    parent_id = page.release_date.strftime("release-%Y-%m-%d")
    new_anchors = []
    new_count = 0

    for feature in page.features:
        source_id = f"{parent_id}#{feature.anchor_id}"
        tracked = db.get_feature_tracking(source_id)

        if tracked is None:
            new_count += 1
            if new_count <= first_run_limit:
                new_anchors.append(feature.anchor_id)

            db.upsert_feature_tracking(
                source_id=source_id,
                parent_id=parent_id,
                feature_type="release_note_feature",
                anchor_id=feature.anchor_id
            )

    # Page is "new" if all features are new (first time seeing this page)
    existing_features = db.get_features_for_parent(parent_id)
    is_new_page = len(existing_features) == len(page.features) and len(new_anchors) > 0

    return (is_new_page, new_anchors)
```

**Step 4: Verify pass, commit**

```bash
git commit -am "feat(scraper): add classify_release_features function"
```

---

## Task 27: Add classify_deploy_changes Function

**Files:**
- Modify: `src/scrapers/instructure_community.py`
- Test: `tests/test_scrapers.py`

**Step 1: Write failing test**

```python
class TestClassifyDeployChanges:
    """Tests for classify_deploy_changes function."""

    def test_new_changes_detected(self, temp_db):
        """Test new changes are detected."""
        from scrapers.instructure_community import (
            DeployNotePage, DeployChange, classify_deploy_changes
        )
        from datetime import datetime

        change = DeployChange("Nav", "Fix", "fix-1", "Updates", "", None, None, None)
        page = DeployNotePage(
            title="Canvas Deploy Notes (2026-02-11)",
            url="http://example.com/deploy",
            deploy_date=datetime(2026, 2, 11),
            beta_date=None, changes=[change], sections={}
        )

        is_new, new_anchors = classify_deploy_changes(page, temp_db, first_run_limit=3)
        assert is_new is True
        assert "fix-1" in new_anchors
```

**Step 2: Run test**

```bash
pytest tests/test_scrapers.py::TestClassifyDeployChanges -v
```

**Step 3: Implement**

```python
def classify_deploy_changes(
    page: DeployNotePage,
    db: "Database",
    first_run_limit: int = 3
) -> Tuple[bool, List[str]]:
    """Classify deploy note changes as new or existing.

    Args:
        page: DeployNotePage with parsed changes.
        db: Database instance for tracking.
        first_run_limit: Max new changes on first run.

    Returns:
        Tuple of (is_new_page, list_of_new_anchor_ids).
    """
    parent_id = page.deploy_date.strftime("deploy-%Y-%m-%d")
    new_anchors = []
    new_count = 0

    for change in page.changes:
        source_id = f"{parent_id}#{change.anchor_id}"
        tracked = db.get_feature_tracking(source_id)

        if tracked is None:
            new_count += 1
            if new_count <= first_run_limit:
                new_anchors.append(change.anchor_id)

            db.upsert_feature_tracking(
                source_id=source_id,
                parent_id=parent_id,
                feature_type="deploy_note_change",
                anchor_id=change.anchor_id
            )

    existing_changes = db.get_features_for_parent(parent_id)
    is_new_page = len(existing_changes) == len(page.changes) and len(new_anchors) > 0

    return (is_new_page, new_anchors)
```

**Step 4: Verify pass, commit**

```bash
git commit -am "feat(scraper): add classify_deploy_changes function"
```

---

# Phase 7: Main Orchestration

## Task 28: Update Main.py Imports

**Files:**
- Modify: `src/main.py`

**Step 1: Add new imports**

```python
from scrapers.instructure_community import (
    InstructureScraper,
    CommunityPost,
    ReleaseNote,
    ChangeLogEntry,
    DiscussionUpdate,
    classify_discussion_posts,
    classify_release_features,
    classify_deploy_changes,
)
from generator.rss_builder import (
    RSSBuilder,
    build_discussion_title,
    format_discussion_description,
    build_release_note_entry,
    build_deploy_note_entry,
)
```

**Step 2: Commit**

```bash
git commit -am "chore(main): add new imports for v1.3.0 features"
```

---

## Task 29: Integrate Classification in Main Workflow

**Files:**
- Modify: `src/main.py`
- Test: `tests/test_main.py`

**Step 1: Write integration test**

```python
class TestV130Integration:
    """Integration tests for v1.3.0 features."""

    def test_discussion_tracking_flow(self, temp_db):
        """Test full discussion tracking flow."""
        from scrapers.instructure_community import CommunityPost, classify_discussion_posts
        from datetime import datetime

        # First run
        posts = [CommunityPost(
            title="Question", url="http://example.com/discussion/100/test",
            content="Content", published_date=datetime.now(),
            comments=2, post_type="question"
        )]
        results1 = classify_discussion_posts(posts, temp_db, first_run_limit=5)
        assert results1[0].is_new is True

        # Second run with more comments
        posts[0].comments = 5
        results2 = classify_discussion_posts(posts, temp_db, first_run_limit=5)
        assert results2[0].is_new is False
        assert results2[0].new_comment_count == 3
```

**Step 2: Update main() function**

Replace the processing section in `main()` (around line 178) with:

```python
        # 4. Process content with v1.3.0 tracking
        logger.info("Processing content with v1.3.0 tracking...")

        # First-run limits
        FIRST_RUN_LIMITS = {
            "question": 5,
            "blog": 5,
            "release_note": 3,
            "deploy_note": 3,
        }

        # Separate items by type
        discussion_posts = []
        release_notes = []
        deploy_notes = []
        other_items = []

        for item in all_items:
            if item.content_type in ("question", "blog"):
                # Convert back to CommunityPost for classification
                post = CommunityPost(
                    title=item.title, url=item.url, content=item.content,
                    published_date=item.published_date,
                    comments=item.comment_count, post_type=item.content_type
                )
                discussion_posts.append(post)
            elif item.content_type == "release_note":
                release_notes.append(item)
            elif item.content_type == "deploy_note":
                deploy_notes.append(item)
            else:
                other_items.append(item)

        # Classify discussion posts
        qa_posts = [p for p in discussion_posts if p.post_type == "question"]
        blog_posts = [p for p in discussion_posts if p.post_type == "blog"]

        qa_updates = classify_discussion_posts(
            qa_posts, db, first_run_limit=FIRST_RUN_LIMITS["question"]
        )
        blog_updates = classify_discussion_posts(
            blog_posts, db, first_run_limit=FIRST_RUN_LIMITS["blog"]
        )

        # Convert updates to ContentItems
        items_to_process = list(other_items)

        for update in qa_updates + blog_updates:
            item = community_post_to_content_item(update.post)
            if not update.is_new:
                item.content_type = f"{update.post.post_type}_updated"
            items_to_process.append(item)

        # TODO: Add release/deploy note classification when parse methods are complete

        logger.info(
            f"  -> {len(qa_updates)} Q&A, {len(blog_updates)} blog "
            f"({len([u for u in qa_updates + blog_updates if u.is_new])} new, "
            f"{len([u for u in qa_updates + blog_updates if not u.is_new])} updated)"
        )

        # Continue with existing enrichment and RSS generation...
        enriched_items = processor.enrich_with_llm(items_to_process)
```

**Step 3: Run tests**

```bash
pytest tests/ -v
```

**Step 4: Commit**

```bash
git commit -am "feat(main): integrate v1.3.0 discussion tracking"
```

---

# Phase 8: Integration Tests

## Task 30: Full Integration Test Suite

**Files:**
- Test: `tests/test_main.py`

**Step 1: Write comprehensive integration test**

```python
class TestV130FullIntegration:
    """Full integration tests for v1.3.0."""

    def test_first_run_then_update_flow(self, temp_db):
        """Test complete first run and update detection flow."""
        from scrapers.instructure_community import (
            CommunityPost, classify_discussion_posts
        )
        from datetime import datetime

        # Simulate 7 Q&A posts
        qa_posts = [
            CommunityPost(
                title=f"Question {i}",
                url=f"http://example.com/discussion/{i}/test",
                content=f"Content {i}",
                published_date=datetime.now(),
                comments=i,
                post_type="question"
            ) for i in range(7)
        ]

        # First run - limit 5
        results1 = classify_discussion_posts(qa_posts, temp_db, first_run_limit=5)
        assert len(results1) == 5
        assert all(r.is_new for r in results1)

        # All 7 tracked
        for i in range(7):
            assert temp_db.get_discussion_tracking(f"question_{i}") is not None

        # Second run - posts 0, 1 have new comments
        qa_posts[0].comments = 10
        qa_posts[1].comments = 15

        results2 = classify_discussion_posts(qa_posts, temp_db, first_run_limit=5)
        assert len(results2) == 2
        assert all(not r.is_new for r in results2)

        # Verify deltas
        deltas = {r.post.url: r.new_comment_count for r in results2}
        assert deltas["http://example.com/discussion/0/test"] == 10
        assert deltas["http://example.com/discussion/1/test"] == 14

    def test_rss_title_formatting(self):
        """Test RSS title formatting for all content types."""
        from generator.rss_builder import build_discussion_title

        # Q&A
        assert build_discussion_title("question", "SSO Help", True) == "[NEW] - Question Forum - SSO Help"
        assert build_discussion_title("question", "SSO Help", False) == "[UPDATE] - Question Forum - SSO Help"

        # Blog
        assert build_discussion_title("blog", "Updates", True) == "[NEW] - Blog - Updates"

        # Release Notes (no source label)
        assert build_discussion_title("release_note", "Canvas Release Notes (2026-02-21)", True) == "[NEW] Canvas Release Notes (2026-02-21)"
        assert build_discussion_title("deploy_note", "Canvas Deploy Notes (2026-02-11)", False) == "[UPDATE] Canvas Deploy Notes (2026-02-11)"
```

**Step 2: Run full test suite**

```bash
pytest tests/ -v --tb=short
```

**Step 3: Final commit**

```bash
git add -A
git commit -m "feat: complete v1.3.0 implementation - discussion tracking and [NEW]/[UPDATE] badges"
```

---

# Summary

| Phase | Tasks | Features |
|-------|-------|----------|
| 1 | 1-5 | Database tables and methods for tracking |
| 2 | 6-11 | Dataclasses for all content types |
| 3 | 12-16 | Scraper parsing methods |
| 4 | 17-19 | LLM summarization for features/changes |
| 5 | 20-24 | RSS formatting functions |
| 6 | 25-27 | Classification and update detection |
| 7 | 28-29 | Main orchestration integration |
| 8 | 30 | Full integration tests |

**Total: 30 tasks**

**First-run limits:**
- Q&A: 5 posts
- Blog: 5 posts
- Release Notes: 3 features
- Deploy Notes: 3 changes

**Note:** Tasks 14-16 provide parsing stubs. Full HTML parsing implementation may require additional iteration based on actual page structure from live Instructure Community pages.
